{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6288a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s24thai/miniconda3/envs/qh_nlp/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Anh ta cần xác định tính chất của các mẫu này', 'Nhiều mưa hơn có nghĩa là nhiều nước mưa đi qua đất hơn']\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"wanhin/vi_text\", split='train')\n",
    "\n",
    "texts = dataset['text']\n",
    "\n",
    "print(texts[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d81a296d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "PAD Token: [PAD]\n",
      "CLS Token: [CLS]\n",
      "SEP Token: [SEP]\n",
      "UNK Token: [UNK]\n",
      "MASK Token: [MASK]\n",
      "Vocabulary size: 10000\n",
      "Tokens (WordPiece): ['Anh', 'ta', 'cần', 'xác', 'định', 'tính', 'chất', 'của', 'các', 'mẫu', 'này']\n",
      "Token IDs (WordPiece): [1100, 1120, 1314, 1339, 1149, 1367, 1468, 959, 974, 2332, 996]\n",
      "Decoded (WordPiece): Anh ta cần xác định tính chất của các mẫu này\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "tokenizer_wordpiece = BertWordPieceTokenizer(lowercase=False)\n",
    "tokenizer_wordpiece.train_from_iterator(texts, vocab_size=10000, min_frequency=2)\n",
    "\n",
    "tokenizer_wordpiece.pad_token = '[PAD]'\n",
    "tokenizer_wordpiece.cls_token = '[CLS]'\n",
    "tokenizer_wordpiece.sep_token = '[SEP]'\n",
    "tokenizer_wordpiece.unk_token = '[UNK]'\n",
    "tokenizer_wordpiece.mask_token = '[MASK]'\n",
    "print(f\"PAD Token: {tokenizer_wordpiece.pad_token}\")\n",
    "print(f\"CLS Token: {tokenizer_wordpiece.cls_token}\")\n",
    "print(f\"SEP Token: {tokenizer_wordpiece.sep_token}\")\n",
    "print(f\"UNK Token: {tokenizer_wordpiece.unk_token}\")\n",
    "print(f\"MASK Token: {tokenizer_wordpiece.mask_token}\")\n",
    "\n",
    "vocab = tokenizer_wordpiece.get_vocab()\n",
    "print(\"Vocabulary size:\", len(vocab))\n",
    "\n",
    "sample_text = texts[0]\n",
    "encoded_sample_wordpiece = tokenizer_wordpiece.encode(sample_text)\n",
    "print(f\"Tokens (WordPiece): {encoded_sample_wordpiece.tokens}\")\n",
    "print(f\"Token IDs (WordPiece): {encoded_sample_wordpiece.ids}\")\n",
    "\n",
    "decoded_sample_wordpiece = tokenizer_wordpiece.decode(encoded_sample_wordpiece.ids)\n",
    "print(f\"Decoded (WordPiece): {decoded_sample_wordpiece}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45297a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "PAD Token: [PAD]\n",
      "CLS Token: [CLS]\n",
      "SEP Token: [SEP]\n",
      "UNK Token: [UNK]\n",
      "MASK Token: [MASK]\n",
      "Vocabulary size: 10000\n",
      "Tokens (BPE): ['anh', 'ta', 'cần', 'xác', 'định', 'tính', 'chất', 'của', 'các', 'mẫu', 'này']\n",
      "Token IDs (BPE): [450, 537, 782, 766, 619, 823, 919, 428, 448, 1681, 467]\n",
      "Decoded (BPE): anh ta cần xác định tính chất của các mẫu này\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer, models, pre_tokenizers, trainers\n",
    "from tokenizers.normalizers import Lowercase\n",
    "\n",
    "tokenizer_bpe = Tokenizer(models.BPE())\n",
    "tokenizer_bpe.normalizer = Lowercase()\n",
    "tokenizer_bpe.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "trainer = trainers.BpeTrainer(vocab_size=10000, min_frequency=2)\n",
    "tokenizer_bpe.train_from_iterator(texts, trainer=trainer)\n",
    "\n",
    "tokenizer_bpe.pad_token = '[PAD]'\n",
    "tokenizer_bpe.cls_token = '[CLS]'\n",
    "tokenizer_bpe.sep_token = '[SEP]'\n",
    "tokenizer_bpe.unk_token = '[UNK]'\n",
    "tokenizer_bpe.mask_token = '[MASK]'\n",
    "print(f\"PAD Token: {tokenizer_wordpiece.pad_token}\")\n",
    "print(f\"CLS Token: {tokenizer_wordpiece.cls_token}\")\n",
    "print(f\"SEP Token: {tokenizer_wordpiece.sep_token}\")\n",
    "print(f\"UNK Token: {tokenizer_wordpiece.unk_token}\")\n",
    "print(f\"MASK Token: {tokenizer_wordpiece.mask_token}\")\n",
    "\n",
    "vocab = tokenizer_wordpiece.get_vocab()\n",
    "print(\"Vocabulary size:\", len(vocab))\n",
    "\n",
    "encoded_sample_bpe = tokenizer_bpe.encode(sample_text)\n",
    "print(f\"Tokens (BPE): {encoded_sample_bpe.tokens}\")\n",
    "print(f\"Token IDs (BPE): {encoded_sample_bpe.ids}\")\n",
    "\n",
    "decoded_sample_bpe = tokenizer_bpe.decode(encoded_sample_bpe.ids)\n",
    "print(f\"Decoded (BPE): {decoded_sample_bpe}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d05ab19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "PAD Token: [PAD]\n",
      "CLS Token: [CLS]\n",
      "SEP Token: [SEP]\n",
      "UNK Token: [UNK]\n",
      "MASK Token: [MASK]\n",
      "Vocabulary size: 10000\n",
      "Tokens (SentencePiece): ['▁Anh', '▁ta', '▁cần', '▁xác', '▁định', '▁tính', '▁chất', '▁của', '▁các', '▁mẫu', '▁này']\n",
      "Token IDs (SentencePiece): [708, 728, 930, 956, 757, 986, 1092, 544, 565, 1969, 591]\n",
      "Decoded (SentencePiece): Anh ta cần xác định tính chất của các mẫu này\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import SentencePieceBPETokenizer\n",
    "\n",
    "tokenizer_sentencepiece = SentencePieceBPETokenizer()\n",
    "\n",
    "tokenizer_sentencepiece.train_from_iterator(texts, vocab_size=10000, min_frequency=2)\n",
    "\n",
    "tokenizer_sentencepiece.pad_token = '[PAD]'\n",
    "tokenizer_sentencepiece.cls_token = '[CLS]'\n",
    "tokenizer_sentencepiece.sep_token = '[SEP]'\n",
    "tokenizer_sentencepiece.unk_token = '[UNK]'\n",
    "tokenizer_sentencepiece.mask_token = '[MASK]'\n",
    "print(f\"PAD Token: {tokenizer_sentencepiece.pad_token}\")\n",
    "print(f\"CLS Token: {tokenizer_sentencepiece.cls_token}\")\n",
    "print(f\"SEP Token: {tokenizer_sentencepiece.sep_token}\")\n",
    "print(f\"UNK Token: {tokenizer_sentencepiece.unk_token}\")\n",
    "print(f\"MASK Token: {tokenizer_sentencepiece.mask_token}\")\n",
    "\n",
    "vocab = tokenizer_wordpiece.get_vocab()\n",
    "print(\"Vocabulary size:\", len(vocab))\n",
    "\n",
    "encoded_sample_sentencepiece = tokenizer_sentencepiece.encode(sample_text)\n",
    "print(f\"Tokens (SentencePiece): {encoded_sample_sentencepiece.tokens}\")\n",
    "print(f\"Token IDs (SentencePiece): {encoded_sample_sentencepiece.ids}\")\n",
    "\n",
    "decoded_sample_sentencepiece = tokenizer_sentencepiece.decode(encoded_sample_sentencepiece.ids)\n",
    "print(f\"Decoded (SentencePiece): {decoded_sample_sentencepiece}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qh_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
